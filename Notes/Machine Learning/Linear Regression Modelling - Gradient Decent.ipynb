{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b61c46",
   "metadata": {},
   "source": [
    "## Problem statement:\n",
    "Given the size of house and the actual prices it is sold for. A project given by the real estate agent is to come up with basic model which can predict the price of a house given its size.\n",
    "\n",
    "For building up the concept of linear regression let us consider we only have 3 data points. Prices of 1000 sqft house is 300000 dollars. 2000sqft house cost 500000 dollars and 3000 sqft house cost 700 dollars. Given this data how to predict the cost of 1200 sqft house.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| -------------------| ------------------------ |\n",
    "| 1.0               | 300                      |\n",
    "| 2.0               | 500                      |\n",
    "| 3.0               | 700                      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b955c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train = (3,)\n",
      "shape of x_train = (3,)\n"
     ]
    }
   ],
   "source": [
    "# importing numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "x_train = np.array([1.0,2.0,3.0])\n",
    "y_train = np.array([300,500,700])\n",
    "print(f\"shape of x_train = {x_train.shape}\")\n",
    "print(f\"shape of x_train = {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa6468",
   "metadata": {},
   "source": [
    "### Cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36acefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to compute cost for every traing set.\n",
    "\n",
    "def compute_cost(x,y,w,b):\n",
    "    '''\n",
    "    x : one D array\n",
    "    y : one D array of target output\n",
    "    w : parameter \n",
    "    b : base term \n",
    "    \n",
    "    '''\n",
    "    m = x.shape[0]\n",
    "    #fwb = np.zeros(m)\n",
    "    error = 0\n",
    "    accumalated_error = 0\n",
    "    for i in range(0,m):\n",
    "        fwb = w * x[i] + b\n",
    "        # diff between predicted and actual\n",
    "        error = (fwb - y[i])**2\n",
    "        accumalated_error +=  error\n",
    "    total_cost = 1 / (2*m) * accumalated_error\n",
    "    return total_cost\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b5730",
   "metadata": {},
   "source": [
    "### Gradient decent - intutition\n",
    "In the previous note we have defined a cost function and tried to guestimate the values for w and b parameter. The actual way of calculating the parameters is using a technique called gradient decent. The intution behind gradient decent is that the cost function based on MSE will always have a miminal Value for J for the given parameter value of w & b, and it is found by computing J value for a randomly selected value and trying to change w & b using a formula and contine this in iteration for some time w & b converges \n",
    "\n",
    "\n",
    "We have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180c4b7",
   "metadata": {},
   "source": [
    "\n",
    "*gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13501660",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent\n",
    "We will implement gradient descent algorithm for one feature. You will need three functions. \n",
    "- `compute_gradient` implementing equation (4) and (5) above\n",
    "- `compute_cost` implementing equation (2) above (code from previous lab)\n",
    "- `gradient_descent`, utilizing compute_gradient and compute_cost\n",
    "\n",
    "Conventions:\n",
    "- The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n",
    "- w.r.t is With Respect To, as in partial derivative of $J(wb)$ With Respect To $b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773078c8",
   "metadata": {},
   "source": [
    "### Compute gradient function \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd55e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(x,y,w,b):\n",
    "    '''\n",
    "    x : one D array\n",
    "    y : one D array of target output\n",
    "    w : parameter \n",
    "    b : base term \n",
    "    \n",
    "    '''\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    f_wb = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495995de",
   "metadata": {},
   "source": [
    "### Plotting of gradients - To be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee3e2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "### Computing Gradient decent\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    '''\n",
    "    x : one D array\n",
    "    y : one D array of target output\n",
    "    w_in  : parameter \n",
    "    b_in  : base term \n",
    "    alpha : learning rate\n",
    "    num_iter : total number of times decent to be done\n",
    "    cost function : funtion to compute the cost for a givne w & b\n",
    "    gradient_funtion : function to compute gradient for a given w & b\n",
    "    \n",
    "    '''\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    m = x.shape[0]\n",
    "    cost_history = []\n",
    "    param_history = []\n",
    "    for i in range(num_iters):\n",
    "        # compute the gradient \n",
    "        dj_dw, dj_db = gradient_function(x,y,w,b)\n",
    "\n",
    "        ## do change of w & b based on learning rate\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        if i<100000:   \n",
    "            ## store the cost\n",
    "            cost_history.append(cost_function(x,y,w,b))\n",
    "            param_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {cost_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    return w, b, cost_history, param_history #return w and J,w history for graphing\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0592feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.23e+05  dj_dw: -1.133e+03, dj_db: -5.000e+02   w:  1.133e+01, b: 5.00000e+00\n",
      "Iteration 1000: Cost 6.55e-01  dj_dw:  1.600e-01, dj_db: -3.636e-01   w:  2.013e+02, b: 9.69785e+01\n",
      "Iteration 2000: Cost 5.91e-02  dj_dw:  4.805e-02, dj_db: -1.092e-01   w:  2.004e+02, b: 9.90924e+01\n",
      "Iteration 3000: Cost 5.33e-03  dj_dw:  1.443e-02, dj_db: -3.281e-02   w:  2.001e+02, b: 9.97274e+01\n",
      "Iteration 4000: Cost 4.81e-04  dj_dw:  4.335e-03, dj_db: -9.855e-03   w:  2.000e+02, b: 9.99181e+01\n",
      "Iteration 5000: Cost 4.34e-05  dj_dw:  1.302e-03, dj_db: -2.960e-03   w:  2.000e+02, b: 9.99754e+01\n",
      "Iteration 6000: Cost 3.92e-06  dj_dw:  3.912e-04, dj_db: -8.893e-04   w:  2.000e+02, b: 9.99926e+01\n",
      "Iteration 7000: Cost 3.53e-07  dj_dw:  1.175e-04, dj_db: -2.671e-04   w:  2.000e+02, b: 9.99978e+01\n",
      "Iteration 8000: Cost 3.19e-08  dj_dw:  3.530e-05, dj_db: -8.024e-05   w:  2.000e+02, b: 9.99993e+01\n",
      "Iteration 9000: Cost 2.88e-09  dj_dw:  1.060e-05, dj_db: -2.410e-05   w:  2.000e+02, b: 9.99998e+01\n",
      "(w,b) found by gradient descent: (200.0000, 99.9999)\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, get_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff383c",
   "metadata": {},
   "source": [
    "Note:  We can see the w & b parameter we have got is 200, ~100, in the previous session we have guestimated this number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e170a70",
   "metadata": {},
   "source": [
    "### Increased learning rate\n",
    "A higher learning rate will cause the GD to diverge. Let us see with an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28fb3496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.63e+06  dj_dw: -1.133e+03, dj_db: -5.000e+02   w:  9.067e+02, b: 4.00000e+02\n",
      "Iteration    1: Cost 1.93e+07  dj_dw:  3.898e+03, dj_db:  1.713e+03   w: -2.212e+03, b:-9.70667e+02\n",
      "Iteration    2: Cost 2.28e+08  dj_dw: -1.340e+04, dj_db: -5.894e+03   w:  8.505e+03, b: 3.74436e+03\n",
      "Iteration    3: Cost 2.69e+09  dj_dw:  4.604e+04, dj_db:  2.025e+04   w: -2.833e+04, b:-1.24586e+04\n",
      "Iteration    4: Cost 3.18e+10  dj_dw: -1.583e+05, dj_db: -6.962e+04   w:  9.828e+04, b: 4.32368e+04\n",
      "Iteration    5: Cost 3.76e+11  dj_dw:  5.440e+05, dj_db:  2.393e+05   w: -3.369e+05, b:-1.48195e+05\n",
      "Iteration    6: Cost 4.44e+12  dj_dw: -1.870e+06, dj_db: -8.225e+05   w:  1.159e+06, b: 5.09793e+05\n",
      "Iteration    7: Cost 5.25e+13  dj_dw:  6.426e+06, dj_db:  2.827e+06   w: -3.982e+06, b:-1.75183e+06\n",
      "Iteration    8: Cost 6.20e+14  dj_dw: -2.209e+07, dj_db: -9.717e+06   w:  1.369e+07, b: 6.02176e+06\n",
      "Iteration    9: Cost 7.33e+15  dj_dw:  7.592e+07, dj_db:  3.340e+07   w: -4.705e+07, b:-2.06974e+07\n",
      "(w,b) found by gradient descent: (-47050141.7033,-20697430.4432)\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10\n",
    "tmp_alpha = 8.0e-1\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, get_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "observe the value of w "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
