{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d812a0a8",
   "metadata": {},
   "source": [
    "## Predicting coffee roasing temp and duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee5bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66ab757",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2)\n",
    "X = rng.random(400).reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64abd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now X is in range of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bc22cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185.31763812  12.69396457]\n",
      " [259.92047498  11.86766377]\n",
      " [231.01357101  14.41424211]\n",
      " [175.3666449   11.72058651]\n",
      " [187.12086467  14.12973206]\n",
      " [225.90586448  12.10024905]\n",
      " [208.40515676  14.17718919]\n",
      " [207.07593089  14.0327376 ]\n",
      " [280.60385359  14.23225929]\n",
      " [202.86935247  12.24901028]\n",
      " [196.70468985  13.54426389]\n",
      " [270.31327028  14.60225577]\n",
      " [192.94979108  15.19686759]\n",
      " [213.57283453  14.27503537]\n",
      " [164.47298664  11.91817423]\n",
      " [177.25750542  15.03779869]\n",
      " [241.7745473   14.89694529]\n",
      " [236.99889634  13.12616959]\n",
      " [219.73805621  13.87377407]\n",
      " [266.38592796  13.25274466]\n",
      " [270.45241485  13.95486775]\n",
      " [261.96307698  13.49222422]\n",
      " [243.4899478   12.8561015 ]\n",
      " [220.58184803  12.36489356]\n",
      " [163.59498627  11.65441652]\n",
      " [244.76317931  13.32572248]\n",
      " [271.19410986  14.84073282]\n",
      " [201.98784315  15.39471508]\n",
      " [229.9283715   14.56353326]\n",
      " [204.97123839  12.28467965]\n",
      " [173.18989704  12.2248249 ]\n",
      " [231.51374483  11.95053142]\n",
      " [152.68795109  14.83198786]\n",
      " [163.42050092  13.30233814]\n",
      " [215.94730737  13.98108963]\n",
      " [218.04195512  15.24937257]\n",
      " [251.30354024  13.79786599]\n",
      " [233.33173846  13.52620597]\n",
      " [280.2428442   12.40650425]\n",
      " [243.01866352  13.72038672]\n",
      " [155.67159152  12.6846    ]\n",
      " [275.16753628  14.63825943]\n",
      " [151.73219763  12.68650532]\n",
      " [151.32372212  14.80986777]\n",
      " [164.89962496  11.72982072]\n",
      " [282.55425133  13.28347952]\n",
      " [192.98305487  11.69605356]\n",
      " [202.59536338  12.96415623]\n",
      " [220.66990639  11.52714187]\n",
      " [169.97498884  12.3395461 ]\n",
      " [209.46811003  12.70921347]\n",
      " [232.79923633  12.64173042]\n",
      " [272.80045636  15.34747983]\n",
      " [158.02370683  12.33623888]\n",
      " [226.00812974  14.58264092]\n",
      " [158.64327123  12.23921074]\n",
      " [211.65721643  14.17476303]\n",
      " [271.94927014  14.96789185]\n",
      " [257.15698108  11.71092857]\n",
      " [281.84592659  13.9585118 ]\n",
      " [161.62563505  12.5197151 ]\n",
      " [233.80180142  13.04256047]\n",
      " [210.29146919  14.71834026]\n",
      " [261.24418195  13.68714792]\n",
      " [256.98089905  13.12401972]\n",
      " [281.55504804  13.92063666]\n",
      " [280.63922766  11.67595077]\n",
      " [269.16350813  13.73750875]\n",
      " [246.34126918  12.2703799 ]\n",
      " [224.07333576  12.6571117 ]\n",
      " [164.23986438  11.51274708]\n",
      " [272.42340268  14.18361726]\n",
      " [177.67793377  12.53127471]\n",
      " [212.85523206  14.77314901]\n",
      " [165.87945639  15.37113932]\n",
      " [277.42795347  12.47864038]\n",
      " [236.50555103  12.94039013]\n",
      " [244.13865151  11.8476644 ]\n",
      " [213.44539383  13.85396321]\n",
      " [234.57422435  14.2711819 ]\n",
      " [270.33648536  12.46500084]\n",
      " [170.68123284  13.06242775]\n",
      " [226.79179994  15.34211504]\n",
      " [245.91825406  14.4537702 ]\n",
      " [281.31680226  12.57097265]\n",
      " [185.02731945  13.19016335]\n",
      " [189.881701    14.10441354]\n",
      " [278.48137931  12.11404597]\n",
      " [219.9229371   14.2103124 ]\n",
      " [216.57898499  15.15497536]\n",
      " [249.48122814  15.02870718]\n",
      " [165.0882734   12.28305401]\n",
      " [158.87007046  14.81727799]\n",
      " [279.98051934  11.55596408]\n",
      " [256.54924192  14.41132479]\n",
      " [272.60521724  12.58154189]\n",
      " [246.49491975  12.44969153]\n",
      " [160.26448201  14.48081625]\n",
      " [155.69875114  14.29837785]\n",
      " [188.26743273  13.44969358]\n",
      " [270.35697577  12.47363152]\n",
      " [213.22379155  12.92019779]\n",
      " [175.70141973  13.39460587]\n",
      " [174.52009415  14.69602997]\n",
      " [233.00092162  12.63252301]\n",
      " [281.36917436  12.88110738]\n",
      " [240.61964926  14.43289491]\n",
      " [185.80556267  11.54705521]\n",
      " [270.50314335  15.32566605]\n",
      " [172.98079126  12.11442084]\n",
      " [208.41010162  13.89027827]\n",
      " [283.51265469  15.35398447]\n",
      " [283.36013339  12.48227766]\n",
      " [230.84923224  13.24347657]\n",
      " [181.23930992  11.76150948]\n",
      " [172.77833046  12.93380692]\n",
      " [161.88293361  12.10295597]\n",
      " [156.0279505   13.99162509]\n",
      " [216.51672478  12.47421201]\n",
      " [221.05707984  13.19778711]\n",
      " [238.9856852   15.23066888]\n",
      " [197.69443437  14.08060613]\n",
      " [179.55375966  15.2595976 ]\n",
      " [233.38848487  12.13498975]\n",
      " [184.70189322  12.13660544]\n",
      " [174.18309465  12.72719543]\n",
      " [261.11450983  13.32823521]\n",
      " [187.41794561  13.17630344]\n",
      " [186.09876106  14.43472966]\n",
      " [157.93546835  12.65691424]\n",
      " [193.6382219   12.22607807]\n",
      " [249.65103076  12.22098945]\n",
      " [190.56498132  11.72590625]\n",
      " [252.00406102  12.95545207]\n",
      " [238.55033302  12.36894441]\n",
      " [152.94302629  12.78967263]\n",
      " [255.17362013  14.84978312]\n",
      " [197.09336712  14.88776311]\n",
      " [156.79710499  13.58845814]\n",
      " [184.7520262   13.25631075]\n",
      " [179.92164592  15.07426649]\n",
      " [190.79357984  15.28116771]\n",
      " [164.72717415  13.21932402]\n",
      " [209.86506596  14.33773917]\n",
      " [196.57800642  13.46997985]\n",
      " [159.51062317  12.74412583]\n",
      " [247.87288291  11.92364557]\n",
      " [212.44231569  12.44690782]\n",
      " [172.34040752  11.98526205]\n",
      " [259.8719002   14.24662808]\n",
      " [201.22657354  13.06656377]\n",
      " [248.34175919  13.9158246 ]\n",
      " [273.66206058  15.17765474]\n",
      " [215.09358488  14.13654103]\n",
      " [223.53014138  12.74114217]\n",
      " [211.22431235  14.38470346]\n",
      " [224.61209061  14.02962547]\n",
      " [215.7548952   15.31191057]\n",
      " [254.8222958   12.02314013]\n",
      " [259.90478358  15.17031031]\n",
      " [260.24885577  12.87243299]\n",
      " [199.6677145   12.47285748]\n",
      " [157.52013585  13.38824538]\n",
      " [264.8148241   14.57524634]\n",
      " [239.39685171  14.88876268]\n",
      " [238.98037311  12.39333366]\n",
      " [258.42593537  12.97008002]\n",
      " [270.15836599  12.80593012]\n",
      " [162.40676333  14.41959278]\n",
      " [164.53231154  14.98085772]\n",
      " [205.6096794   14.6204848 ]\n",
      " [157.09674149  13.67535103]\n",
      " [241.38069604  12.01802052]\n",
      " [232.13370589  12.07209046]\n",
      " [191.03853216  12.96114082]\n",
      " [233.644025    12.02047538]\n",
      " [174.9514637   14.62502635]\n",
      " [246.64321151  13.31682268]\n",
      " [188.07040705  14.26881857]\n",
      " [213.15899445  12.74607869]\n",
      " [268.08223648  12.30736127]\n",
      " [258.57818535  13.97127162]\n",
      " [237.20731698  14.22860981]\n",
      " [251.01659085  15.02378838]\n",
      " [274.27882002  12.52195619]\n",
      " [172.12463765  15.08549633]\n",
      " [177.51695425  12.38785975]\n",
      " [258.70969379  15.36444235]\n",
      " [264.01362166  13.56692157]\n",
      " [200.7060497   15.45420693]\n",
      " [249.36929914  14.01637408]\n",
      " [151.50238376  12.28076712]\n",
      " [151.82138896  15.12816166]\n",
      " [181.92285734  12.18408524]\n",
      " [228.64664273  12.31240743]\n",
      " [223.78183257  15.2991668 ]\n",
      " [266.62767329  12.48051014]\n",
      " [273.68398195  13.09756176]\n",
      " [220.61000617  12.7998907 ]\n",
      " [284.99434167  12.72829382]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
    "X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
    "\n",
    "\n",
    "Y = np.zeros(len(X))\n",
    "    \n",
    "i=0\n",
    "for t,d in X:\n",
    "    y = -3/(260-175)*t + 21\n",
    "    if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
    "        Y[i] = 1\n",
    "    else:\n",
    "        Y[i] = 0\n",
    "    i += 1\n",
    "\n",
    "X = X\n",
    "Y= Y.reshape(-1,1)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ec6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ed4bf",
   "metadata": {},
   "source": [
    "### Normalize Data\n",
    "Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range. \n",
    "The procedure below uses a Keras [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). It has the following steps:\n",
    "- create a \"Normalization Layer\". Note, as applied here, this is not a layer in your model.\n",
    "- 'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
    "- normalize the data.  \n",
    "It is important to apply normalization to any future data that utilizes the learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b026d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)  # learns mean, variance\n",
    "Xn = norm_l(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17cd1810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 2) (200000, 1)\n"
     ]
    }
   ],
   "source": [
    "## increase the training example by tile\n",
    "Xt = np.tile(Xn,(1000,1))\n",
    "Yt= np.tile(Y,(1000,1))   \n",
    "print(Xt.shape, Yt.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79420d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f658c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(2,)),\n",
    "        Dense(3, activation='sigmoid', name = 'layer1'),\n",
    "        Dense(1, activation='sigmoid', name = 'layer2')\n",
    "     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a777b",
   "metadata": {},
   "source": [
    ">**Note 1:** The `tf.keras.Input(shape=(2,)),` specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point.  This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the `model.fit` statement.  \n",
    ">**Note 2:** Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability. This will be described in more detail in a later lab.\n",
    "\n",
    "The `model.summary()` provides a description of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05578551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 3)                 9         \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cecee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to get parameters numbers ; it depends on the input size and number of number of nuerons\n",
    "\n",
    "## EXample layer 1 2 input 3 neuron layer. \n",
    "\n",
    "w_layer = 2 * 3 + 3 * 1 #9\n",
    "b_layer = 1 * 3 + 1 * 1 #4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b57723",
   "metadata": {},
   "source": [
    "Let's examine the weights and biases Tensorflow has instantiated.  The weights $W$ should be of size (number of features in input, number of units in the layer) while the bias $b$ size should match the number of units in the layer:\n",
    "- In the first layer with 3 units, we expect W to have a size of (2,3) and $b$ should have 3 elements.\n",
    "- In the second layer with 1 unit, we expect W to have a size of (3,1) and $b$ should have 1 element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e3cc93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19685084 -0.71965426 -0.45032644]\n",
      " [-0.01329792  1.0376117  -0.5560321 ]]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "print(W1)\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d87db50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.67317605]\n",
      " [ 0.32792222]\n",
      " [ 1.0070678 ]]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(W2)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b2bf869",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model \n",
    "model.compile(\n",
    "loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de197a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0130\n",
      "Epoch 2/10\n",
      "6250/6250 [==============================] - 10s 2ms/step - loss: 0.0088\n",
      "Epoch 3/10\n",
      "6250/6250 [==============================] - 10s 2ms/step - loss: 0.0078\n",
      "Epoch 4/10\n",
      "6250/6250 [==============================] - 9s 1ms/step - loss: 0.0057\n",
      "Epoch 5/10\n",
      "6250/6250 [==============================] - 10s 2ms/step - loss: 0.0065\n",
      "Epoch 6/10\n",
      "6250/6250 [==============================] - 12s 2ms/step - loss: 0.0053\n",
      "Epoch 7/10\n",
      "6250/6250 [==============================] - 13s 2ms/step - loss: 0.0053\n",
      "Epoch 8/10\n",
      "6250/6250 [==============================] - 10s 2ms/step - loss: 0.0065\n",
      "Epoch 9/10\n",
      "6250/6250 [==============================] - 10s 2ms/step - loss: 0.0047\n",
      "Epoch 10/10\n",
      "6250/6250 [==============================] - 6s 973us/step - loss: 0.0070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dcfae91880>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xt,Yt, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4ab23",
   "metadata": {},
   "source": [
    "#### Epochs and batches\n",
    "In the `fit` statement above, the number of `epochs` was set to 10. This specifies that the entire data set should be applied during training 10 times.  During training, you see output describing the progress of training that looks like this:\n",
    "```\n",
    "Epoch 1/10\n",
    "6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n",
    "```\n",
    "The first line, `Epoch 1/10`, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line `6250/6250 [====` is describing which batch has been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b6d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.4147566  -21.81393    -26.398855  ]\n",
      " [-17.372026    -0.47571805 -21.975174  ]]\n",
      "[-20.760496 -22.34693   -4.144626]\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "print(W1)\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78283fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-62.24599 ]\n",
      " [-62.254955]\n",
      " [ 61.625168]]\n",
      "[-9.784358]\n"
     ]
    }
   ],
   "source": [
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(W2)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ec20eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## store and set the weight to model to avoid rerunning every time the model to get the weights\n",
    "\n",
    "W1 = np.array([\n",
    "    [-8.94,  0.29, 12.89],\n",
    "    [-0.17, -7.34, 10.79]] )\n",
    "b1 = np.array([-9.87, -9.28,  1.01])\n",
    "W2 = np.array([\n",
    "    [-31.38],\n",
    "    [-27.86],\n",
    "    [-32.79]])\n",
    "b2 = np.array([15.54])\n",
    "model.get_layer(\"layer1\").set_weights([W1,b1])\n",
    "model.get_layer(\"layer2\").set_weights([W2,b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "810bbd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n",
      "predictions = \n",
      " [[9.625139e-01]\n",
      " [3.031606e-08]]\n"
     ]
    }
   ],
   "source": [
    "## Making predictions\n",
    "X_test = np.array([\n",
    "    [200,13.9],  # postive example\n",
    "    [200,17]])   # negative example\n",
    "X_testn = norm_l(X_test)\n",
    "predictions = model.predict(X_testn)\n",
    "print(\"predictions = \\n\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347af62",
   "metadata": {},
   "source": [
    "To convert the probability to decision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a51c929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decisions = \n",
      "[[1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "yhat = np.zeros_like(predictions)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] >= 0.5:\n",
    "        yhat[i] = 1\n",
    "    else:\n",
    "        yhat[i] = 0\n",
    "print(f\"decisions = \\n{yhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40761504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
